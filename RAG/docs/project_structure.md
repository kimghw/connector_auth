# RAG í”„ë¡œí† íƒ€ìž… í”„ë¡œì íŠ¸ êµ¬ì¡°

## ë””ë ‰í† ë¦¬ êµ¬ì¡°
```
RAG/
â”œâ”€â”€ rag_service.py             # ðŸ”¥ ê³µê°œ ì„œë¹„ìŠ¤ API (ì™¸ë¶€ ì¸í„°íŽ˜ì´ìŠ¤)
â”‚
â”œâ”€â”€ core/                      # ë‚´ë¶€ êµ¬í˜„ (private)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ _document_manager.py   # ë‚´ë¶€: ë¬¸ì„œ ê´€ë¦¬ ë¡œì§
â”‚   â”œâ”€â”€ _text_processor.py     # ë‚´ë¶€: í…ìŠ¤íŠ¸ ì²˜ë¦¬
â”‚   â”œâ”€â”€ _chunker.py            # ë‚´ë¶€: ì²­í‚¹ ë¡œì§
â”‚   â”œâ”€â”€ _embeddings.py         # ë‚´ë¶€: ìž„ë² ë”© ìƒì„±
â”‚   â””â”€â”€ _search_engine.py      # ë‚´ë¶€: ê²€ìƒ‰ ì—”ì§„
â”‚
â”œâ”€â”€ models/                    # ë°ì´í„° ëª¨ë¸
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ document.py           # Document í´ëž˜ìŠ¤
â”‚   â”œâ”€â”€ chunk.py              # Chunk í´ëž˜ìŠ¤
â”‚   â””â”€â”€ search_result.py      # SearchResult í´ëž˜ìŠ¤
â”‚
â”œâ”€â”€ storage/                   # ë°ì´í„° ì €ìž¥ ê´€ë ¨
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ database.py           # SQLite ì—°ê²° ë° ì¿¼ë¦¬
â”‚   â”œâ”€â”€ vector_store.py       # ë²¡í„° ì €ìž¥ì†Œ (NumPy/FAISS)
â”‚   â””â”€â”€ file_storage.py       # íŒŒì¼ ì‹œìŠ¤í…œ ê´€ë¦¬
â”‚
â”œâ”€â”€ utils/                     # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py             # ì„¤ì • ê´€ë¦¬
â”‚   â”œâ”€â”€ logger.py             # ë¡œê¹…
â”‚   â””â”€â”€ text_utils.py         # í…ìŠ¤íŠ¸ ì²˜ë¦¬ ìœ í‹¸
â”‚
â”œâ”€â”€ tests/                     # í…ŒìŠ¤íŠ¸ ì½”ë“œ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_document.py
â”‚   â”œâ”€â”€ test_chunker.py
â”‚   â”œâ”€â”€ test_embeddings.py
â”‚   â””â”€â”€ test_search.py
â”‚
â”œâ”€â”€ examples/                  # ì‚¬ìš© ì˜ˆì œ
â”‚   â”œâ”€â”€ simple_example.py     # ê¸°ë³¸ ì‚¬ìš©ë²•
â”‚   â”œâ”€â”€ batch_processing.py   # ë°°ì¹˜ ì²˜ë¦¬ ì˜ˆì œ
â”‚   â””â”€â”€ search_demo.py        # ê²€ìƒ‰ ë°ëª¨
â”‚
â”œâ”€â”€ data/                      # ë°ì´í„° ì €ìž¥ ë””ë ‰í† ë¦¬
â”‚   â”œâ”€â”€ documents/            # ì›ë³¸ ë¬¸ì„œ ì €ìž¥
â”‚   â”œâ”€â”€ embeddings/           # ìž„ë² ë”© ë²¡í„° ì €ìž¥
â”‚   â””â”€â”€ database/             # SQLite DB íŒŒì¼
â”‚
â”œâ”€â”€ notebooks/                 # Jupyter ë…¸íŠ¸ë¶ (í…ŒìŠ¤íŠ¸ìš©)
â”‚   â”œâ”€â”€ 01_data_processing.ipynb
â”‚   â”œâ”€â”€ 02_embedding_test.ipynb
â”‚   â””â”€â”€ 03_search_evaluation.ipynb
â”‚
â”œâ”€â”€ requirements.txt           # ì˜ì¡´ì„± íŒ¨í‚¤ì§€
â”œâ”€â”€ setup.py                   # íŒ¨í‚¤ì§€ ì„¤ì •
â”œâ”€â”€ config.yaml               # ì„¤ì • íŒŒì¼
â”œâ”€â”€ README.md                 # í”„ë¡œì íŠ¸ ì„¤ëª…
â””â”€â”€ .gitignore               # Git ì œì™¸ íŒŒì¼
```

## ì£¼ìš” ëª¨ë“ˆ ì„¤ëª…

### 1. rag_service.py (ê³µê°œ API)
```python
"""RAG ì‹œìŠ¤í…œ ê³µê°œ ì„œë¹„ìŠ¤ API"""

__all__ = [
    'upload_document',
    'list_documents',
    'get_document',
    'delete_document',
    'process_document',
    'get_processing_status',
    'search',
    'evaluate_search_quality'
]

# ê³µê°œ ì„œë¹„ìŠ¤ í•¨ìˆ˜
def upload_document(file_path: str) -> str:
    """ë¬¸ì„œ ì—…ë¡œë“œ (ê³µê°œ API)"""
    from core._document_manager import DocumentManager
    dm = DocumentManager()
    return dm._upload(file_path)

def search(query: str, top_k: int = 5) -> List[SearchResult]:
    """ê²€ìƒ‰ ìˆ˜í–‰ (ê³µê°œ API)"""
    from core._search_engine import SearchEngine
    se = SearchEngine()
    return se._search(query, top_k)

# ... ê¸°íƒ€ ê³µê°œ í•¨ìˆ˜ë“¤
```

### 2. core/_document_manager.py (ë‚´ë¶€ êµ¬í˜„)
```python
class DocumentManager:
    def __init__(self, storage_path: str):
        """ë¬¸ì„œ ê´€ë¦¬ìž ì´ˆê¸°í™”"""

    def _upload(self, file_path: str) -> str:
        """ë‚´ë¶€: ë¬¸ì„œ ì—…ë¡œë“œ êµ¬í˜„"""

    def _list_all(self) -> List[Document]:
        """ë‚´ë¶€: ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ"""

    def _get_by_id(self, doc_id: str) -> Document:
        """ë‚´ë¶€: ë¬¸ì„œ ì¡°íšŒ"""

    def _delete(self, doc_id: str) -> bool:
        """ë‚´ë¶€: ë¬¸ì„œ ì‚­ì œ"""

    def _validate_file(self, file_path: str):
        """ë‚´ë¶€: íŒŒì¼ ê²€ì¦"""
```

### 3. core/_text_processor.py (ë‚´ë¶€ êµ¬í˜„)
```python
class TextProcessor:
    def _extract_from_pdf(self, file_path: str) -> str:
        """ë‚´ë¶€: PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ"""

    def _extract_from_txt(self, file_path: str) -> str:
        """ë‚´ë¶€: í…ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸°"""

    def _clean(self, text: str) -> str:
        """ë‚´ë¶€: í…ìŠ¤íŠ¸ ì •ì œ"""

    def _normalize(self, text: str) -> str:
        """ë‚´ë¶€: í…ìŠ¤íŠ¸ ì •ê·œí™”"""
```

### 4. core/_chunker.py (ë‚´ë¶€ êµ¬í˜„)
```python
import tiktoken

class TextChunker:
    def __init__(self, max_tokens: int = 512, overlap_tokens: int = 50):
        """í† í° ê¸°ë°˜ ì²­ì»¤ ì´ˆê¸°í™”"""
        self.encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")

    def _create_chunks(self, text: str) -> List[Dict]:
        """ë‚´ë¶€: í† í° ê¸°ë°˜ ì²­í¬ ìƒì„±"""

    def _count_tokens(self, text: str) -> int:
        """ë‚´ë¶€: í† í° ìˆ˜ ê³„ì‚°"""
```

### 5. core/_embeddings.py (ë‚´ë¶€ êµ¬í˜„)
```python
class EmbeddingGenerator:
    def __init__(self, api_key: str, model: str = "text-embedding-ada-002"):
        """OpenAI ìž„ë² ë”© ìƒì„±ê¸° ì´ˆê¸°í™”"""

    def _generate_single(self, text: str) -> np.ndarray:
        """ë‚´ë¶€: ë‹¨ì¼ ìž„ë² ë”© ìƒì„±"""

    def _generate_batch(self, texts: List[str]) -> np.ndarray:
        """ë‚´ë¶€: ë°°ì¹˜ ìž„ë² ë”© ìƒì„± (ìµœëŒ€ 20ê°œ)"""
```

### 6. core/_search_engine.py (ë‚´ë¶€ êµ¬í˜„)
```python
class SearchEngine:
    def __init__(self, vector_store):
        """ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”"""

    def _search(self, query: str, top_k: int = 5) -> List[SearchResult]:
        """ë‚´ë¶€: ê²€ìƒ‰ ìˆ˜í–‰"""

    def _calculate_similarity(self, query_embedding: np.ndarray,
                            corpus_embeddings: np.ndarray) -> np.ndarray:
        """ë‚´ë¶€: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
```

## ì‚¬ìš© ì˜ˆì œ

### ê¸°ë³¸ ì‚¬ìš©ë²• (ê³µê°œ API ì‚¬ìš©)
```python
import rag_service

# 1. ë¬¸ì„œ ì—…ë¡œë“œ
doc_id = rag_service.upload_document("example.pdf")
print(f"Document uploaded: {doc_id}")

# 2. ë¬¸ì„œ ì²˜ë¦¬ (ì½œë°±ìœ¼ë¡œ ì§„í–‰ìƒí™© ì¶”ì )
def progress_callback(step, progress):
    print(f"{step}: {progress}%")

status = rag_service.process_document(doc_id, callback=progress_callback)
print(f"Processing status: {status.status}")

# 3. ê²€ìƒ‰
results = rag_service.search("PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ë°©ë²•", top_k=5)
for result in results:
    print(f"Score: {result.similarity_score:.3f}")
    print(f"Content: {result.content[:200]}...")
    print(f"Source: {result.document_name}\n")

# 4. ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ
documents = rag_service.list_documents()
for doc in documents:
    print(f"{doc.id}: {doc.filename} ({doc.status})")

# 5. ê²€ìƒ‰ í’ˆì§ˆ í‰ê°€
eval_data = [
    {"query": "PDF ì²˜ë¦¬", "relevant_chunks": ["chunk_123", "chunk_456"]}
]
metrics = rag_service.evaluate_search_quality(eval_data)
print(f"Precision@5: {metrics['precision_at_5']}")
print(f"MRR: {metrics['mrr']}")
```

## ì„¤ì • íŒŒì¼ (config.yaml)
```yaml
# ë¬¸ì„œ ì²˜ë¦¬ ì„¤ì •
document:
  max_file_size_mb: 10
  allowed_formats: ["pdf", "txt"]
  storage_path: "./data/documents"

# ì²­í‚¹ ì„¤ì • (í† í° ê¸°ë°˜)
chunking:
  max_tokens: 512
  overlap_tokens: 50
  preserve_sentence_boundary: true

# ìž„ë² ë”© ì„¤ì • (OpenAI)
embedding:
  model: "text-embedding-ada-002"
  batch_size: 20  # OpenAI API ì œí•œ ê³ ë ¤
  api_key_env: "OPENAI_API_KEY"  # í™˜ê²½ë³€ìˆ˜ëª…

# ê²€ìƒ‰ ì„¤ì •
search:
  top_k: 5
  similarity_threshold: 0.7

# ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
database:
  path: "./data/database/rag.db"
```

## ì˜ì¡´ì„± íŒ¨í‚¤ì§€ (requirements.txt)
```
# í•µì‹¬ íŒ¨í‚¤ì§€
numpy>=1.21.0
pandas>=1.3.0
scikit-learn>=0.24.0

# ë¬¸ì„œ ì²˜ë¦¬
PyPDF2>=3.0.0
pdfplumber>=0.9.0
python-docx>=0.8.11

# OpenAI API
openai>=1.0.0
tiktoken>=0.5.0  # í† í° ì¹´ìš´íŒ…

# ë²¡í„° ì €ìž¥
faiss-cpu>=1.7.0

# ë°ì´í„°ë² ì´ìŠ¤
sqlite3

# ìœ í‹¸ë¦¬í‹°
pyyaml>=5.4
python-dotenv>=0.19.0
tqdm>=4.62.0


# ê°œë°œ/í…ŒìŠ¤íŠ¸
pytest>=7.0.0
black>=22.0.0
flake8>=4.0.0
```